{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce2afe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91cb1f7e59f4ab785e1ad145bb2fb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/train-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2140d798ab074761b8743f5464c71f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e075a01d01a14326809794970589b674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de4871ce77e4d0595e6b6907f65117a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
      "\n",
      "Epoch [1/5], Step [100/600], Loss: 0.2801\n",
      "Epoch [1/5], Step [200/600], Loss: 0.3913\n",
      "Epoch [1/5], Step [300/600], Loss: 0.1741\n",
      "Epoch [1/5], Step [400/600], Loss: 0.2433\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1173\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1230\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1507\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0699\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0951\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1296\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0462\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0807\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0442\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0575\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0464\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0438\n",
      "Epoch [3/5], Step [500/600], Loss: 0.1352\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0787\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0553\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0487\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0801\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0665\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0420\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0736\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0311\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0392\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0209\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0295\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0332\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0371\n",
      "Accuracy of the network on the 10000 test images: 97.83 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4f0840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 0.6505\n",
      "Epoch [10/60], Loss: 0.5286\n",
      "Epoch [15/60], Loss: 0.4788\n",
      "Epoch [20/60], Loss: 0.4582\n",
      "Epoch [25/60], Loss: 0.4494\n",
      "Epoch [30/60], Loss: 0.4455\n",
      "Epoch [35/60], Loss: 0.4434\n",
      "Epoch [40/60], Loss: 0.4422\n",
      "Epoch [45/60], Loss: 0.4413\n",
      "Epoch [50/60], Loss: 0.4405\n",
      "Epoch [55/60], Loss: 0.4398\n",
      "Epoch [60/60], Loss: 0.4390\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Hyper-parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Toy dataset\n",
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042], \n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827], \n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)\n",
    "\n",
    "# Linear regression model\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    inputs = torch.from_numpy(x_train)\n",
    "    targets = torch.from_numpy(y_train)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Plot the graph\n",
    "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
    "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "896e0ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 2.3020\n",
      "Epoch [1/5], Step [200/600], Loss: 2.1719\n",
      "Epoch [1/5], Step [300/600], Loss: 2.0471\n",
      "Epoch [1/5], Step [400/600], Loss: 1.9569\n",
      "Epoch [1/5], Step [500/600], Loss: 1.9203\n",
      "Epoch [1/5], Step [600/600], Loss: 1.8539\n",
      "Epoch [2/5], Step [100/600], Loss: 1.7661\n",
      "Epoch [2/5], Step [200/600], Loss: 1.7283\n",
      "Epoch [2/5], Step [300/600], Loss: 1.6129\n",
      "Epoch [2/5], Step [400/600], Loss: 1.6729\n",
      "Epoch [2/5], Step [500/600], Loss: 1.5502\n",
      "Epoch [2/5], Step [600/600], Loss: 1.4619\n",
      "Epoch [3/5], Step [100/600], Loss: 1.4590\n",
      "Epoch [3/5], Step [200/600], Loss: 1.4283\n",
      "Epoch [3/5], Step [300/600], Loss: 1.3431\n",
      "Epoch [3/5], Step [400/600], Loss: 1.4156\n",
      "Epoch [3/5], Step [500/600], Loss: 1.3012\n",
      "Epoch [3/5], Step [600/600], Loss: 1.2355\n",
      "Epoch [4/5], Step [100/600], Loss: 1.2479\n",
      "Epoch [4/5], Step [200/600], Loss: 1.1439\n",
      "Epoch [4/5], Step [300/600], Loss: 1.0892\n",
      "Epoch [4/5], Step [400/600], Loss: 1.1904\n",
      "Epoch [4/5], Step [500/600], Loss: 1.0888\n",
      "Epoch [4/5], Step [600/600], Loss: 1.1490\n",
      "Epoch [5/5], Step [100/600], Loss: 1.1040\n",
      "Epoch [5/5], Step [200/600], Loss: 1.0502\n",
      "Epoch [5/5], Step [300/600], Loss: 1.0257\n",
      "Epoch [5/5], Step [400/600], Loss: 0.9591\n",
      "Epoch [5/5], Step [500/600], Loss: 1.0604\n",
      "Epoch [5/5], Step [600/600], Loss: 1.0082\n",
      "Accuracy of the model on the 10000 test images: 82.9000015258789 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 28 * 28    # 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset (images and labels)\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Logistic regression model\n",
    "model = nn.Linear(input_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "# nn.CrossEntropyLoss() computes softmax internally\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape images to (batch_size, input_size)\n",
    "        images = images.reshape(-1, input_size)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8201acf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "w:  Parameter containing:\n",
      "tensor([[-0.3292,  0.3220, -0.3108],\n",
      "        [-0.3880, -0.0990,  0.4201]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([ 0.1262, -0.0251], requires_grad=True)\n",
      "loss:  1.3933056592941284\n",
      "dL/dw:  tensor([[ 0.1114,  0.6061, -0.7900],\n",
      "        [-0.0413, -0.0345,  0.8664]])\n",
      "dL/db:  tensor([ 0.0689, -0.0240])\n",
      "loss after 1 step optimization:  1.3758056163787842\n",
      "Files already downloaded and verified\n",
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3f25654f52ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;31m# You can then use the prebuilt data loader.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0mcustom_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n\u001b[0m\u001b[1;32m    155\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                                            shuffle=True)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[1;32m    108\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# ================================================================== #\n",
    "#                         Table of Contents                          #\n",
    "# ================================================================== #\n",
    "\n",
    "# 1. Basic autograd example 1               (Line 25 to 39)\n",
    "# 2. Basic autograd example 2               (Line 46 to 83)\n",
    "# 3. Loading data from numpy                (Line 90 to 97)\n",
    "# 4. Input pipline                          (Line 104 to 129)\n",
    "# 5. Input pipline for custom dataset       (Line 136 to 156)\n",
    "# 6. Pretrained model                       (Line 163 to 176)\n",
    "# 7. Save and load model                    (Line 183 to 189) \n",
    "\n",
    "\n",
    "# ================================================================== #\n",
    "#                     1. Basic autograd example 1                    #\n",
    "# ================================================================== #\n",
    "\n",
    "# Create tensors.\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 \n",
    "\n",
    "\n",
    "# ================================================================== #\n",
    "#                    2. Basic autograd example 2                     #\n",
    "# ================================================================== #\n",
    "\n",
    "# Create tensors of shape (10, 3) and (10, 2).\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "print ('w: ', linear.weight)\n",
    "print ('b: ', linear.bias)\n",
    "\n",
    "# Build loss function and optimizer.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# Forward pass.\n",
    "pred = linear(x)\n",
    "\n",
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())\n",
    "\n",
    "# Backward pass.\n",
    "loss.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "\n",
    "# 1-step gradient descent.\n",
    "optimizer.step()\n",
    "\n",
    "# You can also perform gradient descent at the low level.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.item())\n",
    "\n",
    "\n",
    "# ================================================================== #\n",
    "#                     3. Loading data from numpy                     #\n",
    "# ================================================================== #\n",
    "\n",
    "# Create a numpy array.\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Convert the numpy array to a torch tensor.\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "# Convert the torch tensor to a numpy array.\n",
    "z = y.numpy()\n",
    "\n",
    "\n",
    "# ================================================================== #\n",
    "#                         4. Input pipeline                           #\n",
    "# ================================================================== #\n",
    "\n",
    "# Download and construct CIFAR-10 dataset.\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True, \n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)\n",
    "\n",
    "# Fetch one data pair (read data from disk).\n",
    "image, label = train_dataset[0]\n",
    "print (image.size())\n",
    "print (label)\n",
    "\n",
    "# Data loader (this provides queues and threads in a very simple way).\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "# Actual usage of the data loader is as below.\n",
    "for images, labels in train_loader:\n",
    "    # Training code should be written here.\n",
    "    pass\n",
    "\n",
    "\n",
    "# ================================================================== #\n",
    "#                5. Input pipeline for custom dataset                 #\n",
    "# ================================================================== #\n",
    "\n",
    "# You should build your custom dataset as below.\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "        # 1. Initialize file paths or a list of file names. \n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset.\n",
    "        return 0 \n",
    "\n",
    "# You can then use the prebuilt data loader. \n",
    "custom_dataset = CustomDataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "# ================================================================== #\n",
    "#                        6. Pretrained model                         #\n",
    "# ================================================================== #\n",
    "\n",
    "# Download and load the pretrained ResNet-18.\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# If you want to finetune only the top layer of the model, set as below.\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the top layer for finetuning.\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example.\n",
    "\n",
    "# Forward pass.\n",
    "images = torch.randn(64, 3, 224, 224)\n",
    "outputs = resnet(images)\n",
    "print (outputs.size())     # (64, 100)\n",
    "\n",
    "\n",
    "# ================================================================== #\n",
    "#                      7. Save and load the model                    #\n",
    "# ================================================================== #\n",
    "\n",
    "# Save and load the entire model.\n",
    "torch.save(resnet, 'model.ckpt')\n",
    "model = torch.load('model.ckpt')\n",
    "\n",
    "# Save and load only the model parameters (recommended).\n",
    "torch.save(resnet.state_dict(), 'params.ckpt')\n",
    "resnet.load_state_dict(torch.load('params.ckpt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92a9e8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/600], Loss: 0.6200\n",
      "Epoch [1/2], Step [200/600], Loss: 0.3149\n",
      "Epoch [1/2], Step [300/600], Loss: 0.3489\n",
      "Epoch [1/2], Step [400/600], Loss: 0.0991\n",
      "Epoch [1/2], Step [500/600], Loss: 0.1530\n",
      "Epoch [1/2], Step [600/600], Loss: 0.1861\n",
      "Epoch [2/2], Step [100/600], Loss: 0.1137\n",
      "Epoch [2/2], Step [200/600], Loss: 0.1693\n",
      "Epoch [2/2], Step [300/600], Loss: 0.1804\n",
      "Epoch [2/2], Step [400/600], Loss: 0.0638\n",
      "Epoch [2/2], Step [500/600], Loss: 0.1195\n",
      "Epoch [2/2], Step [600/600], Loss: 0.1197\n",
      "Test Accuracy of the model on the 10000 test images: 97.71 %\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "learning_rate = 0.003\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Bidirectional recurrent neural network (many-to-one)\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection \n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = BiRNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7bf8995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.1492\n",
      "Epoch [1/5], Step [200/600], Loss: 0.1518\n",
      "Epoch [1/5], Step [300/600], Loss: 0.0897\n",
      "Epoch [1/5], Step [400/600], Loss: 0.0479\n",
      "Epoch [1/5], Step [500/600], Loss: 0.0596\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1403\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0767\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0528\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0086\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0132\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0071\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0099\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0169\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0044\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0556\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0384\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0592\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0153\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0176\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0052\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0066\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0104\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0166\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0095\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0211\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0162\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0205\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0339\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0195\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0129\n",
      "Test Accuracy of the model on the 10000 test images: 98.81 %\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "294ce47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/600], Loss: 0.3211\n",
      "Epoch [1/2], Step [200/600], Loss: 0.2472\n",
      "Epoch [1/2], Step [300/600], Loss: 0.1219\n",
      "Epoch [1/2], Step [400/600], Loss: 0.2896\n",
      "Epoch [1/2], Step [500/600], Loss: 0.2507\n",
      "Epoch [1/2], Step [600/600], Loss: 0.1655\n",
      "Epoch [2/2], Step [100/600], Loss: 0.1260\n",
      "Epoch [2/2], Step [200/600], Loss: 0.0513\n",
      "Epoch [2/2], Step [300/600], Loss: 0.1476\n",
      "Epoch [2/2], Step [400/600], Loss: 0.0312\n",
      "Epoch [2/2], Step [500/600], Loss: 0.1655\n",
      "Epoch [2/2], Step [600/600], Loss: 0.0084\n",
      "Test Accuracy of the model on the 10000 test images: 98.03 %\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8180db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
